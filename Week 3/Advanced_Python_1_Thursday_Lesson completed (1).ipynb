{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"PqWvepZ4x6BI"},"source":["# Coding Temple's Data Analytics Course\n","---\n","## Advanced Python Day 4: Intro to Pandas"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wMBgsgq_x6BJ"},"source":["## Tasks Today:\n","\n","0) <b>Pre-Work</b> <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; a) Numpy Random Sampling\n","\n","1) <b>Pandas</b> <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; a) Importing <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; b) Tabular Data Structures <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - from_dict() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - read_csv() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; c) <b>In-Class Exercise #1</b> <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; d) Accessing Data <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Indexing <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - df.loc <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - keys() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Slicing a DataFrame <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; e) Built-In Methods <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - head() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - tail() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - shape <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - describe() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - sort_values() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - .columns <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; f) <b>In-Class Exercise #2</b> <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; g) Filtration <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Conditionals <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Subsetting <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; h) Column Transformations <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Generating a New Column w/Data <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - User Defined Function <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp; i) Aggregations <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - Type of groupby() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - mean() <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - groupby() w/Multiple Columns <br>\n"," &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; - drop_duplicates() <br>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KfiRpaplx6BK"},"source":["## Numpy Random Sampling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nlt9BhPZx6BK"},"outputs":[],"source":["import numpy as np\n","\n","np.random.seed(42) # My random numbers will never change. Allows for reproducibility\n","\n","# A single call generates a single random number\n","print(f'Here is a random number: {np.random.uniform()}')\n","\n","# You can also pass in some bounds(parameters)\n","print(f'Here is a random number between 1 and 1 million: {np.random.uniform(1,1e6)}')\n","\n","# You can also generate a bunch of random numbers at one time\n","print(f'Here is a 3x3 Matrix with random numbers between 1 and 1 million: \\n{np.random.uniform(1,1e6, (3,3))}')\n","\n","# Instead of floats, let's generate some random integer values\n","print(f\"Here are some random integers: {np.random.randint(0,10,4)}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zpBhbyZFx6BL"},"source":["## Pandas <br>\n","\n","<p>Pandas is a flexible data analysis library built on top of NumPy that is excellent for working with tabular data. It is currently the de-facto standard for Python-based data analysis, and fluency in Pandas will do wonders for your productivity and frankly your resume. It is one of the fastest ways of getting from zero to answer in existence. </p>\n","\n","<ul>\n","    <li>Pandas is a Python module, written in C. The Pandas module is a high performance, highly efficient, and high level data analysis library. It allows us to work with large sets of data called dataframes.</li>\n","    <li>Series is a one-dimensional labeled array capable of holding data of any type (integer, string, float, python objects, etc.)</li>\n","    <li>Dataframe = Spreadsheet (has column headers, index, etc.)</li>\n","</ul>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o2bh21Dfx6BL"},"source":["### Importing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMHJpwOgx6BL"},"outputs":[],"source":["# Pandas is aliased as pd across the board. This is the industry standard\n","import pandas as pd # This is the industry standard"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Hnd5YOmAx6BL"},"source":["### Tabular data structures <br>\n","<p>The central object of study in Pandas is the DataFrame, which is a tabular data structure with rows and columns like an excel spreadsheet. The first point of discussion is the creation of dataframes both from native Python dictionaries, and text files through the Pandas I/O system.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r3I4eHu0x6BM","scrolled":true},"outputs":[],"source":["names = [\n","    'Alice', 'Bob',\n","    'James', 'Beth',\n","    'John', 'Sally',\n","    'Richard', 'Lauren',\n","    'Brandon', 'Sabrina'\n","]\n","ages = np.random.randint(18,35,len(names))\n","\n","my_dict = {\n","    'names' : names,\n","    'ages' : ages\n","}\n","my_dict"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rZO9g3cAx6BM"},"source":["##### from_dict()\n","\n","<p>Let's convert our not-so-useful-for-analysis dict into a Pandas dataframe. We can use the from_dict function to do this easily using Pandas:</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKuyL-Ppx6BM","scrolled":false},"outputs":[],"source":["# df is another industry standard, it is short for dataframe. When create a dataframe, you will see me use this QUITE often\n","df = pd.DataFrame.from_dict(my_dict)\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"u0JKUJ_1x6BM"},"source":["##### read_csv()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFAu0Y4Ox6BM","scrolled":true},"outputs":[],"source":["# You can also bring in textual data and create a dataframe(table) out of it\n","df1 = pd.read_csv(r'boston_marathon_2017 (1).csv')\n","df1"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FmRu69fRx6BM"},"source":["### In-Class Exercise #1 - Read in Boston Red Sox Hitting Data <br>\n","<p>Use the pandas read_csv() method to read in the statistics from the two files yesterday.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsqZZOWBx6BN"},"outputs":[],"source":["bs2017 = pd.read_csv(r'redsox_2017_hitting (1).txt')\n","bs2018 = pd.read_csv(r'redsox_2018_hitting (1).txt')\n","\n","display(bs2017)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["display(bs2018)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QxylPLdqx6BN"},"source":["### Accessing Data <br>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SiPfZJXLx6BN"},"source":["##### Indexing\n","\n","<p>You can directly select a column of a dataframe just like you would a dict. The result is a Pandas 'Series' object.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jeMl0HLwx6BN"},"outputs":[],"source":["# These are the same object, different names\n","# Both are 1-D data structures, or vectors\n","print(df['names'])\n","print(my_dict['names'])\n","\n","# Let's take a look at the type of data structure we have\n","print(type(df['names']))\n","print(type(my_dict['names']))\n","\n","# The difference between a vector and a matrix, using shape\n","print(df.shape)\n","print(df.names.shape)\n","\n","# Index a series based on the numeric value of the index\n","print(df.names[0])\n","print(df.names[5])\n","\n","# We can also index into the dataframe object itself using numerical indexing\n","# We can point to the dataframe object, then ask for the row.\n","df['ages'][0]\n","\n","# What if I wanted to return multiple columns?\n","df[['ages', 'names']]\n","\n","# Pandas makes it really easy to change the TYPE of the data with no problems at all\n","df['ages_float'] = df['ages'].astype(float)\n","df['avg_age_random_calculation'] = df['ages'] / df['ages_float']\n","df"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"s90FyMDpx6BN"},"source":["##### df.loc"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"w75W3Zwgx6BN"},"source":["Along the horizontal dimension, rows of Pandas DataFrames are Row objects. You will notice there is a third column present in the DataFrame - this is the $\\textit{index}$. It is automatically generated as a row number, but can be reassigned to a column of your choice using the DataFrame.set_index(colname) method. We can use it to access particular Pandas $\\textit{rows}$, which are also Series objects:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# df.set_index('names')\n","# Grab the first row of data using the index for that row\n","print(df.loc[0])\n","print(type(df.loc[0]))\n","\n","# Grab multiple values using the .loc function\n","# Multi-level indexing, with nested lists\n","print(df.loc[[0,1,2]][['names', 'ages_float']])\n","print(df.loc[0:2][:])\n","\n","# Use df.loc to set a user-defined index\n","# This is using multi-level indexing with a nested list\n","new_df = df.loc[[0,1,2]][:].set_index('names')\n","display(new_df)\n","\n","# However, slicing will always be easier\n","display(df.loc[0:3].set_index('names'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMy4nVW8x6BN"},"outputs":[],"source":["# Conditional statement:\n","new_df[new_df.ages == 29]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tse1fd8px6BO"},"source":["##### keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VE5l1gu2x6BO"},"outputs":[],"source":["my_dict.keys()\n","\n","# Keys also works to gather the columns of a dataframe object:\n","print(df.keys())\n","print(bs2017.keys())\n","# We can also access this information using the .columns attribute\n","print(df.columns)\n","print(bs2018.columns)\n","\n","# We can also cast these objects to a list!\n","bs2018.columns.to_list()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OOQ0yaf6x6BO"},"source":["##### Slicing a DataFrame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgiB04ijx6BO"},"outputs":[],"source":["print(f'Full Dataframe:\\n{df}')\n","print(f'\\n Full Dataframe as a sliced copy: \\n {df[:]}')\n","print(f'\\nFirst row of data: \\n{df[:1]}')\n","print(f'\\nFifth row of data to the end of the dataset: \\n{df[5:]}')\n","print(f'\\n2nd through the 5th row of data: \\n{df[2:6]}')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2nampPdVx6BO"},"source":["### Built-In Methods <br>\n","\n","<p>These are methods that are frequently used when using Pandas to make your life easier. It is possible to spend a whole week simply exploring the built-in functions supported by DataFrames in Pandas. Here however, we will simply highlight a few ones that might be useful, to give you an idea of what's possible out of the box with Pandas:</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xQejC43Qx6BO"},"source":["##### .head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cuMTsFrx6BO","scrolled":true},"outputs":[],"source":["#df.head() -- Allows us to view the beginning of a dataframe. The .head() function can also accept an integer value to return \n","# More data than the default 5 rows\n","display(bs2017.head())\n","\n","# Edit the number of rows returned\n","display(bs2017.head(20))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XgmusPHRx6BO"},"source":["##### .tail()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaTIFaI3x6BP","scrolled":false},"outputs":[],"source":["# Same thing as above, but unlike a head, the tail starts at the end\n","# Takes the same positional arguments as our .head() method.\n","display(bs2017.tail())\n","\n","# Edit number of rows returned to us:\n","display(bs2017.tail(20))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"d6tId4Cex6BP"},"source":["##### .describe()\n","Probably one of the most important methods to understand. .describe collects all summary statistics in one dataframe object, allowing easy viewing and understanding\n","Of the count of values, mean, standard deviation, minimum value, maximum value, and inner-quartile ranges"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dioJUjFex6BP"},"outputs":[],"source":["# Describe by default, applies summary statistics to the numerical columns present within a dataframe object.\n","display(df.describe())\n","\n","# Describe can also be run on a single column, or a Series object\n","df.ages.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AJ4EoRHTx6BP"},"outputs":[],"source":["# Describe to look at the summary statistics of the object columns\n","# To do so, we can use the exclude parameter within the .describe() method\n","bs2017.describe(exclude='number')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BdGOry9dx6BP"},"source":["##### .sort_values()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5wb7Auvx6BP"},"outputs":[],"source":["# sort_values is used to sort the values based on a label\n","display(df)\n","display(df.sort_values('names'))\n","\n","# What if I want to sort my values and then reset the index?\n","display(df.sort_values('names', ascending=False).reset_index(drop=True))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jhtouJwox6BP"},"source":["##### .isnull()\n","\n","This method applies a boolean mask across the DataFrame object, returning True for any NaN(Null) values and False for all others."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2_2e6MGsx6BQ"},"outputs":[],"source":["# Method to return the boolean mask across the entire dataframe object\n","bs2017.isnull()\n","\n","# How can we view if there is a null value in a specific column?\n","bs2017.isnull().sum()\n","bs2017['Rk'][bs2017['Rk'] == 'NaN']"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FqUnElhFx6BQ"},"source":["##### .nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMtLOGnhx6BQ"},"outputs":[],"source":["# Provides us with a total number of unique values that are present within a dataframe object\n","print(bs2017.nunique())\n","\n","# We can also call this function on a specific column or Series object as well\n","# The return from this is a singular integer value\n","print('\\n',bs2017['Name'].nunique())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4CGusxi0x6BQ"},"source":["##### .info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ADF1_SvBx6BQ"},"outputs":[],"source":["# To provide useful information on each column present within a dataframe object\n","# This includes stuff like the number of null values, the data types, and the count of rows/columns\n","df.info()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8ouVdkmUx6BQ"},"source":["##### .shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BvrlnLjjx6BR"},"outputs":[],"source":["# This is an attribute of the dataframe object\n","# A dataframe is nothing more than a class that is built in the Pandas library to handle data\n","# We call attributes of a class using dot notation.\n","print(df.shape)\n","\n","# The series class has the same attribute and can be accessed in the exact same way!\n","df['ages_float'].shape\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9PjIeDfXx6BR"},"source":["##### .columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc2vTLg9x6BR"},"outputs":[],"source":["# .columns is another attribute we can call on a dataframe object\n","# This will show all the columns present within a dataframe\n","print(df.columns)\n","\n","# What kind of datatype is this object?\n","print(type(df.columns))\n","\n","# Another cool thing about the columns is that each is an attribute of the dataframe\n","# Meaning we can call to them using our dot notation, as we have already seen.\n","print(df.ages)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xBrChITwx6BR"},"source":["### In-Class Exercise #2 - Describe & Sort Boston Red Sox Hitting Data <br>\n","<p>Take the data that you read in earlier from the Red Sox csv's and use the describe method to understand the data better. Compare the two years and decide which team is having the better year. Then sort the values based on Batting Average.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VFNeaBbox6BR"},"outputs":[],"source":["print(f'The difference between the years 2017 and 2018 total bases is: {sum(bs2017[\"TB\"]) - sum(bs2018[\"TB\"])}')\n","print(f\"2018 was a better year for total bases for the Boston Red Sox by: {sum(bs2018['TB']) - sum(bs2017['TB'])}\")\n","\n","# sort the values based on a column:\n","display(bs2018.sort_values('BA'))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k7h9gIlnx6BS"},"source":["### Filtration <br>\n","<p>Let's look at how to filter dataframes for rows that fulfill a specific conditon.</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T8-9s7Krx6BS"},"source":["##### Conditionals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JLf2V2P1x6BS"},"outputs":[],"source":["# Boolean mask (conditional statement) returns True or False\n","conditional_mask = df['ages'] >= 25\n","conditional_mask"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YmKv4I8Jx6BS"},"source":["##### Subsetting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jKWi6zqZx6BS"},"outputs":[],"source":["display(df[conditional_mask])\n","\n","df['names'][conditional_mask]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yPUJBBpYx6BS"},"source":["### Column Transformations <br>\n","<p>Rarely, if ever, will the columns in the original raw dataframe read from CSV or database table be the ones you actually need for your analysis. You will spend lots of time constantly transforming columns or groups of columns using general computational operations to produce new ones that are functions of the old ones. Pandas has full support for this: Consider the following dataframe containing membership term and renewal number for a group of customers:</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UaxwJvVx6BT"},"outputs":[],"source":["# Generate fake data\n","np.random.seed(42) # Random seed to keep data the same\n","customer_id = np.random.randint(1000,1100,10)\n","renewal_hbr = np.random.randint(0,10,10)\n","customer_dict = {1: 0.5, 0: 1}\n","\n","# Example of feature engineering. We are going to create a new feature withing the data form another source of data\n","term_in_years = [customer_dict[key] for key in np.random.randint(0,2,10)]\n","\n","# Combine them all into a single dictonary object:\n","random_data = {\n","    'customer_id' : customer_id.astype(str),\n","    'Renewal HBR' : renewal_hbr,\n","    'term_in_years' : term_in_years\n","}\n","\n","# Now, we can create a dataframe from this!\n","customers = pd.DataFrame.from_dict(random_data)\n","customers"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["customers.info()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# An example of a column tranformation.\n","# This is important to the world of DA\n","customers['customer_id'] = customers['customer_id'].astype(int)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jcjv0K3hx6BT"},"source":["##### Feature Engineering a New Column w/Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZURqEcnpx6BT"},"outputs":[],"source":["# dataframe['column_name'] = Some calculation done on another column or list of equal length\n","customers['customer_tenure'] = customers['Renewal HBR'] * customers['term_in_years']\n","\n","# What if I wanted to augment the ID column to be the id + 1?\n","# Pandas will iterate over the column FOR YOU\n","customers['aug_id'] = customers['customer_id'] + 1\n","display(customers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fbTWNByLx6BT"},"source":["#### Dropping a column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HKDEWQj2x6BT"},"outputs":[],"source":["# Creating a duplicate column in my dataset:\n","customers['customer_id_1'] = customers['customer_id']\n","display(customers)\n","\n","# Let's go ahead and try to drop this column!\n","customers.drop('customer_id_1')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-ygrLP9Wx6BT"},"source":["##### Axis? What's That all about?\n","##### This: \n","![atext](https://i.stack.imgur.com/dcoE3.jpg)  \n","If you check the [documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html), you'll see that DataFrame.drop() has \"axis = 0\" as a default. We need to explicitly (remember the Zen of Python?) tell pandas to look for the column we want to drop from the column axis, which is column 1."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wo14CfSDx6BT"},"outputs":[],"source":["# Now that we are armed with this info, let's try again!\n","customers.drop('customer_id_1', axis = 1)\n","\n","# Let's double check our dataframe!\n","customers\n","\n","# How can I fix it?\n","# Preferred method: save the dataframe to another dataframe variable so that you don't overwrite your source of truth.\n","df2 = customers.drop('customer_id_1', axis = 1)\n","display(df2)\n","\n","# The other method is to drop from the original and overwrite the dataframe in-place.\n","customers.drop('customer_id_1', axis = 1, inplace = True)\n","display(customers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hDxKY6xLx6BT"},"source":["#### Renaming a column"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jivmrq8gx6BU"},"outputs":[],"source":["customers.rename(columns= {'Renewal HBR' : 'renewal_hbr'}, inplace=True)\n","display(customers)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CMz6ny9Yx6BU"},"source":["##### User Defined Function\n","\n","If what you want to do to a column that can't be represented by simple mathematical operations, you can write your own $\\textit{user defined function}$ with the full customizability available in Python and any external Python packages, then map it directly onto a column. Let's add some ages to our customer dataframe, and then classify them into our custom defined grouping scheme:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BlDbp56Kx6BU","scrolled":true},"outputs":[],"source":["# Random seed to create the new column for ages\n","np.random.seed(42)\n","\n","# Instantiates a new column in our dataframe\n","customers['ages'] = np.random.randint(18,70,10)\n","\n","# User-defined function\n","def make_age_groups(age:int):\n","    if 10 <= age < 20:\n","        return 'Teenager'\n","    elif age < 35:\n","        return 'Young Adult'\n","    elif age < 65:\n","        return 'Adult'\n","    else:\n","        return \"Senior\"\n","\n","# This method uses the .apply function to create a column\n","customers['age_group_apply'] = customers['ages'].apply(make_age_groups)\n","customers\n","\n","# We can also do the exact same thing using list comp\n","customers['age_group'] = [make_age_groups(age) for age in customers['ages']]\n","customers"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"L2LwkJMcx6BU"},"source":["As a last example I'll show here how you would use an apply function to create a UDF that depends on $\\textit{more than one}$ column:\n","<li>UDF = User Defined Function</li>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xv5LWuMFx6BU"},"outputs":[],"source":["def make_loyalty_age_group(row):\n","    age = row['ages']\n","    tenure = row['customer_tenure']\n","    \n","    if 10 <= age < 20:\n","        age_group = 'Teenager'\n","    elif age < 35:\n","        age_group = 'Young Adult'\n","    elif age < 65:\n","        age_group =  'Adult'\n","    else:\n","        age_group = \"Senior\"\n","    \n","    if tenure > 2.0:\n","        make_loyalty_age_group = f'Loyal {age_group}'\n","    else:\n","        make_loyalty_age_group = f'New {age_group}'\n","    \n","    return make_loyalty_age_group\n","\n","customers['loyalty_age_group'] = customers.apply(make_loyalty_age_group, axis=1)\n","customers"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CZkb4U7Qx6BV"},"source":["### In-Class Exercise #3 - Create Your Own UDF <br>\n","<p>Using the Boston Red Sox data, create your own UDF which creates a new column called 'All-Star' and puts every player with either a batting average over .280 or an on base percentage of over .360 with a result of 'Yes' in the column and 'No' if not.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SuH7oPLDx6BV"},"outputs":[],"source":["\"\"\"\n","    Name  BA OBP AllStar\n","    --------------------\n","    Name .233 .360 Yes\n","    Name .150 .288 No\n","\"\"\"\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Iv5OqVdax6BV"},"source":["### Aggregations <br>\n","<p>The raw data plus some transformations is generally only half the story. Your objective is to extract actual insights and actionable conclusions from the data, and that means reducing it from potentially billions of rows to some summary statistics via aggregation functions.</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"BcO7vWjkx6BV"},"source":["##### groupby() <br>\n","<p>The .groupby() function is in some ways a 'master' aggregation.</p> \n","\n","<p>Data tables will usually reserve one column as a primary key - that is, a column for which each row has a unique value. This is to facilitate access to the exact rows of a data table that a user wants to view. The other columns will often have repeated values, such as the age groups in the above examples. We can use these columns to explore the data using the Pandas API:</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6u8PUpkx6BV"},"outputs":[],"source":["# Groupby with the column intact as the column/key\n","# Requires a form of aggregation function to be passed through!\n","display(customers.groupby('age_group').count())\n","\n","# Use a groupby function to specify which columns we want to return\n","display(customers.groupby('age_group', as_index=False).count()[['customer_id', 'age_group']])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qMFiKXIYx6BW"},"source":["##### Type of groupby()\n","\n","<p>The result is a new dataframe, the columns of which all contain the counts of the grouped field. Notice the type of a grouped dataframe:</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZSDIwA7x6BW"},"outputs":[],"source":["print(type(customers), '-> Regular DataFrame object')\n","print(type(customers.groupby('age_group')), '-> Groupby Object')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fPgK9Ioxx6BW"},"source":["<p>This is because simply grouping data doesn't quite make sense without an aggregation function like count() to pair with. In this case, we're counting occurances of the grouped field, but that's not all we can do. We can take averages, standard deviations, mins, maxes and much more! Let's see how this works a bit more:</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GU7pydjQx6BW"},"source":["##### mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_rqYZwtax6BW"},"outputs":[],"source":["# display(customers.groupby('ages').mean()['customer_tenure'])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"H8neGTnjx6BW"},"source":["##### groupby() w/Multiple Columns\n","\n","<p>We end up with the average age of the groups in the last column, the average tenure in the tenure column, and so on and so forth. You can even split the groups more finely by passing a list of columns to group by:</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"47gkkIXCx6BW"},"outputs":[],"source":["customers.groupby(['age_group', 'ages']).count().sort_values('ages', ascending=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mJ2e7tFIx6BW"},"source":["##### drop_duplicates()\n","\n","<p>Drops all duplicates from the current dataframe</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hV0HhAlx6BX"},"outputs":[],"source":["customers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TxICQyzgx6BX","scrolled":true},"outputs":[],"source":["customer_copy = customers.drop_duplicates('renewal_hbr').reset_index(drop=True)\n","customer_copy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZJdWSCmkx6BX"},"outputs":[],"source":["# What if I wanted to save this to a file?\n","customer_copy.to_csv('customers.csv', index=False)\n","\n","pd.read_csv(r'boston_marathon_2017 (1).csv')"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"m8DgGF0Ex6BX"},"source":["<p>Thus the groupby operation allows you to rapidly make summary observations about the state of your entire dataset at flexible granularity. In one line above, we actually did something very complicated - that's the power of the dataframe. In fact, the process often consists of several iterative groupby operations, each revealing greater insight than the last - if you don't know where to start with a dataset, try a bunch of groupbys!</p>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bqwqFGFox6BX"},"source":["### Homework Excersise #1 - Find the Total Number of Runs and RBIs for the Red Sox <br>\n","<p>Get total number of home runs and rbi's</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4u3ig10xx6BX","scrolled":true},"outputs":[],"source":["# step 1: Add a new column with the key 'Team' and all column values should be 'BOS'\n","\n","# step 2: Group by the 'Team' column and get total home runs and rbi's\n","\n","# Produce data for both 2017 and 2018 (ie print both seperated by a newline character \\n)\n","\n","\"\"\"\n","TEAM    HR   RBI\n","----------------\n","BOS     144  538\n","\"\"\"\n","import pandas as pd\n","\n","bs2017 = pd.read_csv(r'redsox_2017_hitting (1).txt')\n","bs2018 = pd.read_csv(r'redsox_2018_hitting (1).txt')\n","\n","bs2017['Team'] = 'BOS'\n","bs2018['Team'] = 'BOS'\n","\n","grouped_data_2017 = bs2017.groupby('Team').sum()[['HR', 'RBI']]\n","grouped_data_2018 = bs2018.groupby('Team').sum()[['HR', 'RBI']]\n","\n","print(\"2017:\")\n","print(grouped_data_2017)\n","\n","print(\"\\n2018:\")\n","print(grouped_data_2018)\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
